# %% [markdown]
# Demo: End-to-End ML-IDS-Analyzer Pipeline

This notebook demonstrates:

1. Loading the trained Random Forest model and scaler
2. Ingesting new sample data
3. Scaling features
4. Generating probabilities and applying the tuned threshold
5. Reporting performance metrics (classification report, confusion matrix, ROC & PR curves)

---

# %% [markdown]
## 1. Environment Setup

Ensure you have installed the package in editable mode:
```bash
pip install -e .
```

# %%
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    precision_recall_curve,
    auc
)
from ml_ids_analyzer.config import cfg

# for consistent plot styling
sns.set(style="whitegrid")

# %% [markdown]
## 2. Load Artifacts and Config
# %%
# Paths from config
thresh = cfg['model'].get('threshold', 0.5)
model_path  = cfg['paths']['model_file']
scaler_path = cfg['paths']['scaler_file']
new_file    = cfg['data'].get('new_input_file', 'data/new_data.csv')

# Load
model  = joblib.load(model_path)
scaler = joblib.load(scaler_path)

print(f"Loaded model:   {model_path}")
print(f"Loaded scaler:  {scaler_path}")
print(f"Using threshold: {thresh:.3f}")

# %% [markdown]
## 3. Load & Prepare New Data
# %%
df = pd.read_csv(new_file, skipinitialspace=True)
df.columns = df.columns.str.strip()

df.replace([np.inf, -np.inf], pd.NA, inplace=True)
df.fillna(0, inplace=True)

X = df[cfg['features']]
X_scaled = scaler.transform(X)

# %% [markdown]
## 4. Predict & Apply Threshold
# %%
# Compute probabilities
y_proba = model.predict_proba(X_scaled)[:, 1]

# Apply cutoff
y_pred = (y_proba >= thresh).astype(int)

df['Predicted'] = y_pred
df['Probability'] = y_proba

# If true labels exist in new data, load them
y_true = None
if cfg['label_column'] in df.columns:
    y_true = df[cfg['label_column']].values

print(df[['Predicted', 'Probability']].head())

# %% [markdown]
## 5. Performance Metrics & Plots

If true labels are provided, we can evaluate.

# %%
if y_true is not None:
    # Classification report
    print(classification_report(y_true, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix (Demo)')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.3f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

    # Precision-Recall Curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    pr_auc = auc(recall, precision)
    best_idx = np.argmax(2 * precision * recall / (precision + recall + 1e-9))

    plt.figure(figsize=(6, 4))
    plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}')
    plt.scatter(recall[best_idx], precision[best_idx],
                label=f'Best F1 = {2*precision[best_idx]*recall[best_idx]/(precision[best_idx]+recall[best_idx]):.2f} @thr={thresholds[best_idx]:.2f}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precisionâ€“Recall Curve')
    plt.legend()
    plt.show()
else:
    print('No true labels available: skipping evaluation plots.')